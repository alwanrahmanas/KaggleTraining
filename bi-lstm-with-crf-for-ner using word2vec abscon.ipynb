{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4361,"sourceType":"datasetVersion","datasetId":1014}],"dockerImageVersionId":29271,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nprint(keras.__version__)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom math import nan\nfrom keras.callbacks import ModelCheckpoint\n\n!pip install git+https://www.github.com/keras-team/keras-contrib.git\nfrom keras_contrib.layers import CRF\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"4d5be342-abe9-4c95-98fd-c904762ecca9","_uuid":"47570c242f4774c5d670093b822cf6a3f3a91ba9","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources\n\npackages = ['numpy', 'pandas', 'keras', 'keras-contrib', 'tensorflow']\n# Add more packages as needed\n\nfor package in packages:\n    try:\n        version = pkg_resources.get_distribution(package).version\n        print(f\"{package.capitalize()} version: {version}\")\n    except pkg_resources.DistributionNotFound:\n        print(f\"{package.capitalize()} is not installed.\")\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Importing the dataset for named entity recognition model","metadata":{"_cell_guid":"46eeeccc-37ad-4d58-b0c5-71dcbb4b935b","_uuid":"64d208ee42a7c5919fc1f7f0937b7b6fcb9c3ea7","editable":false}},{"cell_type":"code","source":"dframe = pd.read_csv(\"../input/entity-annotated-corpus/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)","metadata":{"_cell_guid":"fab9a876-dd53-4636-a48e-3a87a5ba9370","_uuid":"39f851aae0e531c23978926c6ad9bc3d2c739705","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dframe","metadata":{"_cell_guid":"0049ec5e-3b99-4059-a09c-7395f1fa3ab7","_uuid":"ce5d3d143151edbe4885ea10ca1994fb2687038a","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"_cell_guid":"06f00bc1-4964-46e9-a1ff-aa04d2605cef","_uuid":"d64e89af977c30f36cb50f590645ead8576d17ab","editable":false}},{"cell_type":"code","source":"!pip install ipython-autotime\n%load_ext autotime","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dframe.columns","metadata":{"_cell_guid":"85bfa964-b5c4-44d9-9cdc-9eb15259d7c9","_uuid":"028f9857d08bd173ac2a4b587ebd6975aa3c3e80","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We want word, pos, sentence_idx and tag as an input ","metadata":{"_cell_guid":"e3448ee3-bed0-4029-88a1-3a79abfd23d5","_uuid":"2d4d560f8101eb4da1a90c05cfa30b32b134be10","editable":false}},{"cell_type":"code","source":"dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n       'prev-prev-word', 'prev-shape', 'prev-word',\"pos\"],axis=1)","metadata":{"_cell_guid":"9f70ec09-0452-495d-aa80-a6358c2d30af","_uuid":"45e7b565a3eacf62a3104969026eed96c7440b34","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"_cell_guid":"fbf18f0c-635d-495a-81a2-46cffd0a1141","_uuid":"fc7378e8db3c58bb9f953abf4991b0166a582373","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gensim","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\n\nprint(\"Gensim version:\", gensim.__version__)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim = 512","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf =  pd.read_parquet('https://github.com/alwanrahmanas/ner/blob/main/labeled/372%20clean%20tanpa%20makalah%20sidang%20blablabla.parquet?raw=true')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"_cell_guid":"8d992370-b7b0-46d1-939c-74c567c23162","_uuid":"ab6642a1268a00928aa8651037ed84db5414b5f0","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 'document_id' in df.columns:\n    df.drop('document_id',axis=1,inplace=True)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure all tokens are strings\ndef clean_tokens(token_list):\n    return [str(token) for token in token_list]\n\ndf['tokens'] = df['tokens'].apply(clean_tokens)","metadata":{"_cell_guid":"4d20e995-322b-4928-85e1-38385b8e8753","_uuid":"8d47f338cf116e591449949652de2f09a1d402c3","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate tokens and tags\ntokens = df['tokens'].tolist()\ntags = df['ner_tags'].tolist()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the list of tokens for training Word2Vec\nflat_tokens = [token for sublist in tokens for token in sublist]","metadata":{"_cell_guid":"768177ab-ed36-46d2-ba06-49b708dbe185","_uuid":"744cc63a695f486351f214f272307656c77db95d","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n# Train the Word2Vec model\nword2vec_model = Word2Vec(sentences=tokens, size=300, window=5, min_count=1, workers=4, sg=1)\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access the vocabulary (index to word mapping)\nvocab = word2vec_model.wv.vocab\n\n# Create a word-to-index mapping\nword_index = {word: vocab[word].index for word in vocab}\n\n# Calculate the vocabulary size\nvocab_size = len(vocab)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_words = len(word_index); n_words","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert tokens to sequences of word indices\nsequences = [[word_index[word] for word in sentence] for sentence in tokens]\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Create a label encoder for the tags\ntag_encoder = LabelEncoder()\ntag_encoder.fit([tag for tag_list in tags for tag in tag_list])\ntag_index = {tag: idx for idx, tag in enumerate(tag_encoder.classes_)}\nnum_tags = len(tag_index)\n\n# Convert tags to sequences of indices\ntag_sequences = [[tag_index[tag] for tag in tag_list] for tag_list in tags]\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure all sequences are padded to the same length\nmax_length = max(len(seq) for seq in sequences)\npadded_sequences = pad_sequences(sequences=sequences, maxlen=512, padding='post', value=tag_index['O'])\npadded_tag_sequences = pad_sequences(sequences=tag_sequences, maxlen=512, padding='post', value=tag_index['O'])\n\nprint(\"Vocabulary Size:\", vocab_size)\nprint(\"Tag Index:\", tag_index)\nprint(\"Padded Sequences:\\n\", padded_sequences)\nprint(\"Padded Tag Sequences:\\n\", padded_tag_sequences)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ### s = set()\n# for filename in padded_tag_sequences[2]:\n#     s.add(filename)\n# print(s)\ntag_index","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define X and y variables\nfrom keras.utils import to_categorical\n\nX = padded_sequences\ny = [to_categorical(i, num_classes=7) for i in padded_tag_sequences]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test sets\nsplit_index = int(0.8 * len(X))\nX_train, X_test = X[:split_index], X[split_index:]\ny_train, y_test = y[:split_index], y[split_index:]\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\n# print(\"y_train shape:\", y_train.shape)\n# print(\"y_test shape:\", y_test.shape)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create list of list of tuples to differentiate each sentence from each other","metadata":{"_cell_guid":"ba6f0e4d-4b1e-40fa-a72a-df2502841ae4","_uuid":"e5c4b55af4bce4308671e8a611c65bf64d0f0678","editable":false}},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n                                            s[\"Tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"Sentence #\").apply(agg_func)\n\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped.get(\"Sentence: {}\".format(self.n_sent), None)\n            if s is not None:\n                self.n_sent += 1\n            return s\n        except:\n            return None","metadata":{"_cell_guid":"6cab4e05-47c2-4c5b-bcbb-f899f0639691","_uuid":"198ccf9cc26eca32fd8e17b7f3e41bf0af3612b1","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getter = SentenceGetter(dataset)","metadata":{"_cell_guid":"16006628-505a-4d37-a5f6-db6a428517b4","_uuid":"fb62acd5794185e2d7714e76712af2ed2ccc6c1d","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = getter.sentences","metadata":{"_cell_guid":"5d0f958e-ac9c-41cb-a4bc-0256525d4f21","_uuid":"88c8e7bc1b13eac9cffc3a8c4e17b27f116e5776","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sentences[0])","metadata":{"_cell_guid":"e33a8086-5906-45b4-8091-2ff97a48d180","_uuid":"d8ef748ceed29d08cab896e10e173ffa750d402c","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sequence length:', maxlen)","metadata":{"_cell_guid":"88cb5e40-fe8a-47a9-9be2-a23340c28a5c","_uuid":"2f4479487f075f8c196a7af9ffaf5dd943a1326d","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how long sentences are so that we can pad them\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","metadata":{"_cell_guid":"263e86f4-2605-4b1f-b3df-e3c42f1c9ec4","_uuid":"ef272f6ecc6b8637ebe644b1982f17c41f190039","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in sentences], bins=50)\nplt.show()","metadata":{"_cell_guid":"61e67c66-cd9f-481f-a1ea-a78815dfed47","_uuid":"f25daa9b0767a04690f14ba99ed5730c71cb2100","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[\"Word\"]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(set(dataset[\"Word\"].values))\nwords.append(\"ENDPAD\")","metadata":{"_cell_guid":"4efea2e0-276c-44bb-b0ce-c94ac99cbf91","_uuid":"db68187a5ea707569031c83db34ca4284025bb8b","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_words = len(words); n_words","metadata":{"_cell_guid":"5b3697b0-0e7d-4265-9fd1-8001792e6fa5","_uuid":"139a721c0b0036e9cc97892ab8d3d2c3ea0ebafa","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fix the tags","metadata":{"editable":false}},{"cell_type":"code","source":"dataset.head()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = []\nfor tag in set(dataset[\"Tag\"].values):\n    if tag is nan or isinstance(tag, float):\n        tags.append('unk')\n    else:\n        tags.append(tag)\nprint(tags)","metadata":{"_cell_guid":"2dc974c3-e5df-45dd-9aff-f8ec43e52399","_uuid":"b51ed46f6c6956fc92a71033c82d804455a01e3d","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_tags = len(tags); n_tags","metadata":{"_cell_guid":"6cd182c7-7f68-420b-b10f-121da695184e","_uuid":"a426a4acd9481ef8618830c9265f4887d63ec9b5","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting words to numbers and numbers to words**","metadata":{"_cell_guid":"15e92312-4539-490c-a34a-8d8c91e97e0e","_uuid":"8d7134b27205443861225d5820d2ac9e0049e30e","editable":false}},{"cell_type":"code","source":"from future.utils import iteritems\nword2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}\nidx2tag = {v: k for k, v in iteritems(tag2idx)}","metadata":{"_cell_guid":"805f61f0-ec6b-4afc-a1f6-60a5fe015023","_uuid":"b8109f08b55281540b01c07d95666b2f413aa0a8","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2idx['nlp']","metadata":{"_cell_guid":"36b9e479-e006-4d4d-9d0b-fbaf6c89db77","_uuid":"28bf99df1f14a53181bb3eade1ebf68f1d2f8cf8","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag2idx[\"B-TUJUAN\"]","metadata":{"_cell_guid":"7e614013-5dd7-4309-b481-7a12b881d8f6","_uuid":"8bca49f998176bfa05a21ebd22822066063184b1","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx2tag[5]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx2tag","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nX = [[word2idx[w[0]] for w in s] for s in sentences]","metadata":{"_cell_guid":"5cd7d9ad-77ae-49ba-b99e-6429a949dede","_uuid":"b25bb0979a6bbe8c6eb11b05c23109592d318a5b","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2idx","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(X).shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pad_sequences(maxlen=dim, sequences=X, padding=\"post\",value=n_words - 1)","metadata":{"_cell_guid":"c3ff7a53-4e1b-4b82-bf2e-d581dbdc4315","_uuid":"75ab049dc0c8a6408913da28666c21ac6f9385d3","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_idx = [[tag2idx[w[1]] for w in s] for s in sentences]\nprint(sentences[100])\nprint(y_idx[100])","metadata":{"_cell_guid":"5d331c3d-a729-454d-ba92-2f3540e3ecac","_uuid":"50607669ece7f12df2be2836193540a0375d848d","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag2idx","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = pad_sequences(maxlen=dim, sequences=y_idx, padding=\"post\", value=tag2idx[\"O\"])\nprint(y_idx[100])","metadata":{"_cell_guid":"8ee02d57-7dc2-4516-9f3b-42507d1d7369","_uuid":"61183c55adee810a0b161cbdfa2a6024862251a4","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\ny = [to_categorical(i, num_classes=n_tags) for i in y]","metadata":{"_cell_guid":"75b64d49-10a1-411c-85bc-5861a257670e","_uuid":"bd9a39278a2010d51ff4f4a9ccbf0df1d6ee5612","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"_cell_guid":"b33bf24e-22cb-4cf4-b504-e84f0ccc84e5","_uuid":"75c39949b2cce357b15f5c42f73421a4b0116de5","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Keras","metadata":{"editable":false}},{"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport keras as k","metadata":{"_cell_guid":"00a15ead-883e-4348-96df-e86dcb65a945","_uuid":"89375c4a136104777c96a13b02d2810d78bcc1d9","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Keras version","metadata":{"editable":false}},{"cell_type":"code","source":"print(k.__version__)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n **Pay attention to the word embedding size","metadata":{"editable":false}},{"cell_type":"markdown","source":"input = Input(shape=(dim,))\nword_embedding_size = 300\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=dim)(input)\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                           dropout=0.05, \n                           recurrent_dropout=0.05, \n                           kernel_initializer=k.initializers.he_normal()))(model)\nmodel = LSTM(units=word_embedding_size * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer=k.initializers.he_normal())(model)\nmodel = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  # previously softmax output layer\n\ncrf = CRF(n_tags)  # CRF layer\nout = crf(model)  # output","metadata":{"editable":false}},{"cell_type":"code","source":"import keras.backend as K\n\n# Define custom loss functions\ndef focal_loss(y_true, y_pred, gamma=2.0):\n    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n    return -K.sum((1 - y_pred) ** gamma * y_true * K.log(y_pred), axis=-1)\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.0\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return -(2.0 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# Define custom metric function\ndef custom_metric(y_true, y_pred, method='focal'):\n    # You can choose either Focal Loss or Dice Loss here\n    if method == 'focal':\n        loss = focal_loss(y_true, y_pred)\n    elif method == 'dice':\n        loss = dice_loss(y_true, y_pred)\n    return loss\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomCRF(CRF):\n    def call(self, inputs, mask=None, training=None):\n        sequence_lengths = K.sum(K.cast(mask, 'int32'), axis=-1)\n        self.sequence_lengths = sequence_lengths  # Store it for later use\n        return super(CustomCRF, self).call(inputs, mask=mask, training=training)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_tags","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your model\n# Define your model\ninput = Input(shape=(dim,))\nword_embedding_size = 300\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=dim)(input)\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                           dropout=0.05, \n                           recurrent_dropout=0.05, \n                           kernel_initializer='he_normal'))(model)\nmodel = LSTM(units=word_embedding_size * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer='he_normal')(model)\noutput = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # Softmax output layer\n\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your model\nmodel = Model(input, output)","metadata":{"_cell_guid":"82ffe0e4-30f4-49a7-902a-e1942ca59259","_uuid":"d9ec4a0fd80b49f9bb28a25de022ec848113fa9a","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = k.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n# #model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n# # Compile the model with custom loss\n# model = Model(input, out)\n# Compile the model with custom loss and accuracy metric\n# Compile the model with custom loss and custom metric\nmodel.compile(optimizer=adam, loss=custom_metric, metrics=[custom_metric])\n","metadata":{"_cell_guid":"bf8e202b-f374-42c0-9a8f-146d595c8648","_uuid":"93300d6dc8ac67eecd8f99f2ee82d2817daf5f96","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model","metadata":{"editable":false}},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming 'model' is your Keras model\nplot_model(model, to_file='model_visualization.png', show_shapes=True, show_layer_names=True)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the model after each epoch if validation is better","metadata":{"editable":false}},{"cell_type":"code","source":"from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n\n# Define a custom callback to calculate validation accuracy\nclass ValidationAccuracy(Callback):\n    def __init__(self):\n        super(ValidationAccuracy, self).__init__()\n        \n    def on_epoch_end(self, epoch, logs=None):\n        if logs is None:\n            logs = {}\n        val_loss = logs.get('val_loss')\n        if val_loss is None:\n            return\n        val_acc = 1 - val_loss  # Assuming higher val_loss means lower accuracy (change this if it's the opposite)\n        logs['val_acc'] = val_acc\n\n# Define the static file path for saving the best model\nfilepath = \"best_model.hdf5\"\n\n# Create the custom callback instance\nval_accuracy_callback = ValidationAccuracy()\n\n# Create the ModelCheckpoint callback with a static file name\ncheckpoint = ModelCheckpoint(filepath, \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max')\n\n# Create the EarlyStopping callback\nearly_stopping = EarlyStopping(monitor='val_acc', \n                               patience=5, \n                               verbose=1, \n                               mode='max', \n                               restore_best_weights=True)\n\n# Use the callbacks in the training loop\ncallbacks_list = [val_accuracy_callback, checkpoint, early_stopping]\n\n# Example of model fitting with the callbacks\n# history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, \n#                     batch_size=64, callbacks=callbacks_list)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n\n# # Define a custom callback to calculate validation accuracy\n# class ValidationAccuracy(Callback):\n#     def __init__(self, filepath):\n#         super(ValidationAccuracy, self).__init__()\n#         self.filepath = filepath\n        \n#     def on_epoch_end(self, epoch, logs=None):\n#         if logs is None:\n#             logs = {}\n#         val_loss = logs.get('val_loss')\n#         if val_loss is None:\n#             return\n#         val_acc = 1 - val_loss  # Assuming higher val_loss means lower accuracy (change this if it's the opposite)\n#         logs['val_acc'] = val_acc\n\n# # Define the file path with the dynamic name\n# filepath = \"ner-bi-lstm-td-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n\n# # Create the custom callback instance\n# val_accuracy_callback = ValidationAccuracy(filepath)\n\n# # Create the ModelCheckpoint callback\n# checkpoint = ModelCheckpoint(filepath, \n#                              monitor='val_acc', \n#                              verbose=1, \n#                              save_best_only=True, \n#                              mode='max')\n# # Create the EarlyStopping callback\n# early_stopping = EarlyStopping(monitor='val_acc', \n#                                patience=20, \n#                                verbose=1, \n#                                mode='max', \n#                                restore_best_weights=True)\n\n# # Use the callbacks in the training loop\n# callbacks_list = [val_accuracy_callback, checkpoint, early_stopping]\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# <!-- from sklearn.model_selection import KFold\n\n# # Define the number of folds for cross-validation\n# n_splits = 3  # for example, you can adjust this number as needed\n\n# # Initialize KFold cross-validator\n# kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Initialize lists to store training/validation histories\n# training_histories = []\n# validation_histories = []\n\n# # Convert y_train to a NumPy array if it's not already one\n# y_train_array = np.array(y_train)\n\n# # Perform cross-validation\n# for train_index, val_index in kf.split(X_train):\n#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n#     y_train_fold, y_val_fold = y_train_array[train_index], y_train_array[val_index] -->\n\n#     # Fit the model on the current fold\n#     history = model.fit(X_train_fold, np.array(y_train_fold), batch_size=16, epochs=5, \n#                         validation_data=(X_val_fold, np.array(y_val_fold)), verbose=1, \n#                         callbacks=callbacks_list)\n    \n#     # Store training/validation history\n#     training_histories.append(history.history['crf_viterbi_accuracy'])  # Use 'crf_viterbi_accuracy' instead of 'accuracy'\n#     validation_histories.append(history.history['val_crf_viterbi_accuracy'])  # Use 'val_crf_viterbi_accuracy' instead of 'val_accuracy'\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Assuming X_train is your tokenized and padded input data\n# print(\"Shape of X_train:\", X_train.shape)\n\n# # Assuming n_tags is the number of unique tags\n# print(\"Number of unique tags (classes):\", n_tags)\n\n# # Confirm the last dimension of X_train matches the number of classes\n# assert X_train.shape[-1] == n_tags, \"Mismatch between the last dimension of X_train and the number of classes (n_tags)\"\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\n# Define the number of folds for cross-validation\nn_splits = 7  # Adjust this number as needed\n\n# Convert y_train to a NumPy array if it's not already one\ny_train_array = np.array(y_train)\n\n# Initialize KFold cross-validator\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Initialize lists to store training/validation histories\ntraining_histories = []\nvalidation_histories = []\n\n# Perform cross-validation\nfor train_index, val_index in kf.split(X_train):\n    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n    y_train_fold, y_val_fold = y_train_array[train_index], y_train_array[val_index]\n\n    # Fit the model on the current fold\n    history = model.fit(X_train_fold, np.array(y_train_fold), batch_size=64, epochs=100, \n                        validation_data=(X_val_fold, np.array(y_val_fold)), verbose=1, \n                        callbacks=callbacks_list)\n    \n    # Store training/validation history\n    training_histories.append(history.history['custom_metric'])  # Use the name of your custom metric function\n    validation_histories.append(history.history['val_custom_metric'])  # Use the name of your custom metric function\n\n\nplt.show()\n\n# batch size 32, k 9","metadata":{"_cell_guid":"5df5d6eb-2285-4a57-b5a0-d0bcbf79c483","_uuid":"93e241a9304d00294cb0f675f1e42b5dbb6a1c07","scrolled":true,"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming callbacks_list contains ModelCheckpoint callback at index 1\nmodel_checkpoint = callbacks_list[1]\n\n# Access the model from the ModelCheckpoint object\nmodel = model_checkpoint.model\n\n# Save the model using tf.keras.Model.save\nmodel.save(f\"{filepath} Word2Vec 512 64 100 model_checkpoint.h5\")\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accumulate metrics by tag ","metadata":{"editable":false}},{"cell_type":"code","source":"","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the loss for each fold\nplt.figure(figsize=(12, 8))\n\n# Plot training loss for each fold\nfor i, training_loss in enumerate(training_histories):\n    plt.plot(training_loss, label=f'Training Fold {i+1}')\n    \n# Plot validation loss for each fold\nfor i, validation_loss in enumerate(validation_histories):\n    plt.plot(validation_loss, label=f'Validation Fold {i+1}', linestyle='--')\n\nplt.title('Training and Validation Loss for Each Fold')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming training_histories and validation_histories are lists of lists\n# where each inner list contains the loss values for each epoch of a fold\n\n# Initialize lists to hold the final epoch loss values\nfinal_training_losses = []\nfinal_validation_losses = []\n\n# Extract the last epoch loss for each fold\nfor training_loss, validation_loss in zip(training_histories, validation_histories):\n    final_training_losses.append(training_loss[-1])\n    final_validation_losses.append(validation_loss[-1])\n\n# Plotting the final epoch loss for each fold\nplt.figure(figsize=(12, 8))\n\n# Plot final training loss for each fold\nplt.plot(final_training_losses, label='Final Training Loss', marker='o')\n\n# Plot final validation loss for each fold\nplt.plot(final_validation_losses, label='Final Validation Loss', marker='o', linestyle='--')\n\nplt.title('Final Epoch Training and Validation Loss for Each Fold')\nplt.xlabel('Fold')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Save the plot to a file\nplt.savefig('final_epoch_loss_BUS 75 epoch 512 dim 64 batch size k 5 on nSCRD with word2vec plot.png')  # Save as a PNG file","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\n# Load the model\nbest_model = load_model(\"/kaggle/working/nsCRD 512 64 100 model_checkpoint.h5\", custom_objects={'custom_metric': custom_metric})\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = {}\nTN = {}\nFP = {}\nFN = {}\nfor tag in tag2idx.keys():\n    TP[tag] = 0\n    TN[tag] = 0    \n    FP[tag] = 0    \n    FN[tag] = 0    \n\ndef accumulate_score_by_tag(gt, pred):\n    \"\"\"\n    For each tag keep stats\n    \"\"\"\n    if gt == pred:\n        TP[gt] += 1\n    elif gt != 'O' and pred == 'O':\n        FN[gt] +=1\n    elif gt == 'O' and pred != 'O':\n        FP[gt] += 1\n    else:\n        TN[gt] += 1\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single prediction and verbose results","metadata":{"editable":false}},{"cell_type":"code","source":"np.array([X_test[i]])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 9\np = model.predict(np.array([X_test[i]]))\np = np.argmax(p, axis=-1)\ngt = np.argmax(y_test[i], axis=-1)\nprint(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n# for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n#     #\n#     print(\"{:14}: ({:5}): {}\".format(words[w],idx2tag[gt[idx]],tags[pred]))","metadata":{"_uuid":"139269e019aaac2da260b0ee5a676a4c618f6673","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict everything at once","metadata":{"editable":false}},{"cell_type":"code","source":"p = model.predict(np.array(X_test))  ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The output is 3d: sent x word x tag prob (softmax)","metadata":{"editable":false}},{"cell_type":"code","source":"p.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standard Classification Report","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grab the 3d dimension and return the index of the highest probability ... the index matches the tag value\nnp.argmax(p, axis=2)","metadata":{"editable":false}},{"cell_type":"code","source":"np.argmax(p, axis=2)[0]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report=classification_report(np.argmax(y_test, 2).ravel(), np.argmax(p, axis=2).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values()))\nprint(report)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"focal loss sentence level resampling 512 64 epoch 75 k 5\n    B-METODE       0.29      0.21      0.24       511\n    B-TEMUAN       0.10      0.07      0.08       473\n    I-METODE       0.17      0.07      0.10       233\n           O       0.63      0.46      0.53      1275\n    I-TUJUAN       0.48      0.33      0.39     11482\n    B-TUJUAN       0.66      0.25      0.36      3836\n    I-TEMUAN       0.89      0.95      0.92    100462\n\n    accuracy                           0.86    118272\n   macro avg       0.46      0.33      0.38    118272\nweighted avg       0.84      0.86      0.84    118272","metadata":{"editable":false}},{"cell_type":"markdown","source":"BUS 512 64 k=7 \nprecision    recall  f1-score   support\n\n    B-METODE       0.00      0.00      0.00       171\n    B-TEMUAN       0.06      0.01      0.02       152\n    I-METODE       0.25      0.01      0.03        76\n           O       0.47      0.41      0.44       410\n    I-TUJUAN       0.67      0.67      0.67      3582\n    B-TUJUAN       0.72      0.36      0.48      1204\n    I-TEMUAN       0.94      0.96      0.95     32805\n\n    accuracy                           0.90     38400\n   macro avg       0.44      0.35      0.37     38400\nweighted avg       0.89      0.90      0.89     38400","metadata":{"editable":false}},{"cell_type":"markdown","source":"focal loss 512 32 k 9 \nprecision    recall  f1-score   support\n\n    B-TUJUAN       0.84      0.72      0.77       288\n    B-TEMUAN       0.77      0.66      0.71       620\n    B-METODE       0.68      0.38      0.49       671\n    I-METODE       0.89      0.88      0.88      1839\n    I-TEMUAN       0.99      0.99      0.99     14484\n           O       0.99      1.00      1.00    135312\n    I-TUJUAN       0.98      0.98      0.98      4482\n\n    accuracy                           0.99    157696\n   macro avg       0.88      0.80      0.83    157696\nweighted avg       0.99      0.99      0.99    157696\n","metadata":{"editable":false}},{"cell_type":"markdown","source":"focal loss 256, 64, 50\n\n              precision    recall  f1-score   support\n\n    B-TUJUAN       0.73      0.68      0.70       197\n    I-TUJUAN       0.97      0.99      0.98      3065\n    I-TEMUAN       0.98      0.99      0.99     10834\n    B-METODE       0.69      0.21      0.32       498\n    I-METODE       0.87      0.85      0.86      1354\n    B-TEMUAN       0.70      0.51      0.59       453\n           O       0.99      0.99      0.99     48111\n\n    accuracy                           0.98     64512\n   macro avg       0.85      0.74      0.78     64512\nweighted avg       0.98      0.98      0.98     64512","metadata":{"editable":false}},{"cell_type":"markdown","source":" focal loss 2 jam dim 512, epoch 100, batch size 64, k 7 abscon\n precision    recall  f1-score   support\n\n    B-TUJUAN       0.85      0.85      0.85       239\n    I-TUJUAN       0.99      0.99      0.99      3789\n    I-TEMUAN       0.97      0.97      0.97     11325\n    B-METODE       0.73      0.53      0.61       555\n    I-METODE       0.91      0.91      0.91      1430\n    B-TEMUAN       0.87      0.74      0.80       471\n           O       0.99      1.00      0.99    111215\n\n    accuracy                           0.99    129024\n   macro avg       0.90      0.85      0.87    129024\nweighted avg       0.99      0.99      0.99    129024","metadata":{"editable":false}},{"cell_type":"code","source":"# import numpy as np\n\n# prec = [0.69 , 0.0, 0.00, 0, 0.15, 0.08, 0.00]\n# rec = [0.97, 0, 0.00, 0, 0.0, 0.02, 0]\n# f1 = [0.8, 0.0, 0.00, 0, 0.01, 0.03, 0]\n\n# av_prec = np.mean(prec)\n# av_rec = np.mean(rec)\n# av_f1 = np.mean(f1)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Average Precision:\", av_prec)\n# print(\"Average Recall:\", av_rec)\n# print(\"Average F1-score:\", av_f1)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accumulate the scores by tag","metadata":{"editable":false}},{"cell_type":"code","source":"for i, sentence in enumerate(X_test):\n    y_hat = np.argmax(p[i], axis=-1)\n    gt = np.argmax(y_test[i], axis=-1)\n    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How did Classification perform for each tag","metadata":{"editable":false}},{"cell_type":"code","source":"for tag in tag2idx.keys():\n    print(f'tag:{tag}')    \n    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summing all values in the dictionaries\ntotal_TP = sum(TP.values())\ntotal_TN = sum(TN.values())\ntotal_FP = sum(FP.values())\ntotal_FN = sum(FN.values())\n\n# Printing the totals\nprint(\"Total TP:\", total_TP)\nprint(\"Total TN:\", total_TN)\nprint(\"Total FP:\", total_FP)\nprint(\"Total FN:\", total_FN)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ALL METRIC \nimport math\n\nprec= total_TP/(total_TP+total_FP)\nrec = total_TP/(total_TP+total_FN)\nf1=2*prec*rec/(prec+rec)\n\n# Print the metrics\nprint(\"Precision:\", round(prec,3))\nprint(\"Recall:\", round(rec,3))\nprint(\"F1-score:\", round(f1,3))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tanpa layer lstm tambahan:\n\nPrecision: 0.9708141321044547\nRecall: 0.9210784779647055\nF1-score: 0.9452925588125315","metadata":{"editable":false}}]}